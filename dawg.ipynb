{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d437fae",
   "metadata": {},
   "source": [
    "# Сбор связей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56a748fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymorphy2\n",
      "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 1.5 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting pymorphy2-dicts-ru\n",
      "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.2 MB 3.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dawg-python>=0.7.1\n",
      "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
      "Collecting docopt>=0.6\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "Building wheels for collected packages: docopt\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13704 sha256=d510b76f0c13d73b88b18bfcd462f5c157173082f4e8b686e7c1b423e9a51ecf\n",
      "  Stored in directory: /home/kukuruku/.cache/pip/wheels/56/ea/58/ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c\n",
      "Successfully built docopt\n",
      "Installing collected packages: pymorphy2-dicts-ru, dawg-python, docopt, pymorphy2\n",
      "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
     ]
    }
   ],
   "source": [
    "!pip install pymorphy2 pymorphy2-dicts-ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fbbbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870358ac",
   "metadata": {},
   "source": [
    "# Сбор зависимостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2811c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n",
      "520\n",
      "530\n",
      "540\n",
      "550\n",
      "560\n",
      "570\n",
      "580\n",
      "590\n",
      "600\n",
      "610\n",
      "620\n",
      "630\n",
      "640\n",
      "650\n",
      "660\n",
      "670\n",
      "680\n",
      "690\n",
      "700\n",
      "710\n",
      "720\n",
      "730\n",
      "740\n",
      "750\n",
      "760\n",
      "770\n",
      "780\n",
      "790\n",
      "800\n",
      "810\n",
      "820\n",
      "830\n",
      "840\n",
      "850\n",
      "860\n",
      "870\n",
      "880\n",
      "890\n",
      "900\n",
      "910\n",
      "920\n",
      "930\n",
      "940\n",
      "950\n",
      "960\n",
      "970\n",
      "980\n",
      "990\n",
      "1000\n",
      "1010\n",
      "1020\n",
      "Could not create new Document from serialised string.\n",
      "1030\n",
      "Could not create new Document from serialised string.\n",
      "1040\n",
      "1050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28686"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nlp import nlp\n",
    "import torch\n",
    "import stanza\n",
    "import glob\n",
    "from morph import morph\n",
    "from functools import lru_cache\n",
    "\n",
    "\n",
    "def get_stanza_filenames():\n",
    "    return glob.iglob('/mnt/data/**/*.stanza', recursive=True)\n",
    "\n",
    "@lru_cache(2048)\n",
    "def normalize(word, pos):\n",
    "    word = word.replace('-', '')\n",
    "    \n",
    "    for parse in morph.parse(word):\n",
    "        if parse.is_known and pos in parse.tag:\n",
    "            return parse.normal_form\n",
    "        \n",
    "    print(word, pos)\n",
    "\n",
    "    return 'unknown'\n",
    "\n",
    "\n",
    "def collect_rels(rels, words):\n",
    "    heads = (word for word in words if word.upos == \"NOUN\")\n",
    "    \n",
    "    for head in heads:\n",
    "        deps = {normalize(word.text, \"ADJF\") for word in words\n",
    "                if word.head == head.id and word.deprel == \"amod\" and word.upos == \"ADJ\"} - {'unknown'}\n",
    "        \n",
    "        if not deps:\n",
    "            continue\n",
    "        \n",
    "        rel_key = normalize(head.text, \"NOUN\")\n",
    "        \n",
    "        if rel_key in rels:\n",
    "            rels[rel_key] |= deps\n",
    "        else:\n",
    "            rels[rel_key] = deps\n",
    "\n",
    "\n",
    "def process_all():\n",
    "    counters = {}\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for filename in get_stanza_filenames():\n",
    "            if i % 10 == 0: print(i)\n",
    "            i += 1\n",
    "            \n",
    "            with open(filename, 'rb') as file:\n",
    "                try:\n",
    "                    rels = {}\n",
    "                    doc = stanza.Document.from_serialized(file.read())\n",
    "\n",
    "                    for sent in doc.sentences:\n",
    "                        collect_rels(rels, sent.words)\n",
    "                        \n",
    "                    for rel_key in rels:\n",
    "                        cnt = Counter(rels[rel_key])\n",
    "                        \n",
    "                        if rel_key in counters:\n",
    "                            counters[rel_key] += cnt\n",
    "                        else:\n",
    "                            counters[rel_key] = cnt\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "    return counters\n",
    "\n",
    "\n",
    "rels = process_all()\n",
    "\n",
    "len(rels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "177d8017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'необязательный': 1,\n",
       "         'добрачный': 1,\n",
       "         'случайный': 1,\n",
       "         'хороший': 1,\n",
       "         'интимный': 1,\n",
       "         'сам': 1})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rels['секс']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fb7141c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rapidfuzz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmorph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m morph\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrapidfuzz\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JaroWinkler\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m takewhile\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdawg\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rapidfuzz'"
     ]
    }
   ],
   "source": [
    "from morph import morph\n",
    "from rapidfuzz.distance import JaroWinkler\n",
    "from itertools import takewhile\n",
    "import dawg\n",
    "\n",
    "\n",
    "def is_known(word):\n",
    "    for parse in morph.parse(word):\n",
    "        if parse.normal_form.replace('ё', 'е') == word.replace('ё', 'е') and 'nomn' in parse.tag and parse.is_known and ('anim' in parse.tag or 'ADJF' in parse.tag or \"PRTF\" in parse.tag):\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def normalize(word):\n",
    "    for parse in morph.parse(word):\n",
    "        if parse.is_known and ('anim' in parse.tag or 'ADJF' in parse.tag or \"PRTF\" in parse.tag):\n",
    "            return parse.normal_form.replace('ё', 'е')\n",
    "\n",
    "    return word.replace('ё', 'е')\n",
    "\n",
    "\n",
    "# Удалить руты с очен маленьким количеством связей\n",
    "# Удалить редкие депсы\n",
    "def merge_unknown_words_to_known(rels):\n",
    "    dict_roots = [root for root in rels if is_known(root)]\n",
    "    non_dict_roots = [root for root in rels if root not in dict_roots]\n",
    "\n",
    "    print(len(dict_roots), len(non_dict_roots))\n",
    "    \n",
    "    # Удаляем редкие неизвестные слова\n",
    "    # и совсем короткие\n",
    "    # и нормализуемые\n",
    "    for ndr in non_dict_roots:\n",
    "        vals = rels[ndr].values()\n",
    "        \n",
    "        if max(vals) < 3:\n",
    "            print('Удаляем по весу ', ndr, vals)\n",
    "            rels.pop(ndr)\n",
    "            non_dict_roots.remove(ndr)\n",
    "            \n",
    "        if len(ndr) < 3:\n",
    "            print('Удаляем по длине ', ndr, vals)\n",
    "            rels.pop(ndr)\n",
    "            non_dict_roots.remove(ndr)\n",
    "        \n",
    "        normalized_ndr = normalize(ndr)\n",
    "        if normalized_ndr != ndr and normalized_ndr in dict_roots:\n",
    "            print('Сливаем normalized', ndr, normalized_ndr)\n",
    "            rels[normalized_ndr] |= rels[ndr]\n",
    "            rels.pop(ndr) \n",
    "            non_dict_roots.remove(ndr)\n",
    "            \n",
    "        ndr_a = f'{ndr}а';\n",
    "        \n",
    "        if is_known(ndr_a):\n",
    "            print('Сливаем +a', ndr, ndr_a)\n",
    "            rels[ndr_a] = rels.get(ndr_a, Counter())\n",
    "            rels[ndr_a] |= rels[ndr]\n",
    "            rels.pop(ndr)\n",
    "            non_dict_roots.remove(ndr)\n",
    "        \n",
    "            \n",
    "    # Сливаем похожие слова\n",
    "    for ndr in non_dict_roots:\n",
    "        nearest_dr = None\n",
    "        nearest_dr_sim = 0\n",
    "        \n",
    "        for dr in dict_roots:\n",
    "            sim = JaroWinkler.similarity(ndr, dr)\n",
    "            \n",
    "            if sim < 0.96:\n",
    "                continue\n",
    "                \n",
    "            if sim > nearest_dr_sim:\n",
    "                nearest_dr_sim = sim\n",
    "                nearest_dr = dr\n",
    "        \n",
    "        if nearest_dr:\n",
    "            print('Сливаем близкие ', ndr, nearest_dr)\n",
    "            rels[nearest_dr] |= rels[ndr]\n",
    "            rels.pop(ndr)\n",
    "            \n",
    "    print(len(dict_roots), len(non_dict_roots))\n",
    "        \n",
    "\n",
    "def drop_rare_roots(rels):\n",
    "    rare = [root for root in rels if max(rels[root].values()) < 3]\n",
    "    \n",
    "    for r in rare:\n",
    "        print('drop_rare_roots ', r, max(rels[r].values()))\n",
    "        rels.pop(r)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "# merge_unknown_words_to_known(rels)\n",
    "\n",
    "\n",
    "def rels_to_seq(rels):\n",
    "    for root in rels:\n",
    "        if not is_known(root):\n",
    "            continue\n",
    "            \n",
    "        deps = rels[root]\n",
    "        bound = 0\n",
    "        \n",
    "        if len(deps) > 2:\n",
    "            [common1, common2] = deps.most_common(2)\n",
    "            bound = common2[1] / 10\n",
    "    \n",
    "        for dep in deps:\n",
    "#            местоимение\n",
    "            [dep_word, dep_rel, dep_pos] = dep.split(\":\")\n",
    "            if 'Apro' in morph.parse(dep_word)[0].tag:\n",
    "                continue\n",
    "                \n",
    "            if 'блуд' in dep_word or 'мертв' in dep_word or 'твой' in dep_word or 'сбит' in dep_word or 'убит' in dep_word: \n",
    "                continue\n",
    "            \n",
    "            if deps[dep] >= bound:\n",
    "                yield f\"{root.lower().replace('ё', 'е')}:{dep.lower().replace('ё', 'е')}\"\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "completion_dawg = dawg.CompletionDAWG(rels_to_seq(rels))\n",
    "\n",
    "completion_dawg.save('/mnt/data/adj.dawg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "402e7360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='спятившая', tag=OpencorporaTag('PRTF,perf,intr,past,actv femn,sing,nomn'), normal_form='спятить', score=1.0, methods_stack=((DictionaryAnalyzer(), 'спятившая', 2345, 22),))]"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph.parse('спятившая')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "8a9f5ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['зомби:мерзкий:amod:adj',\n",
       " 'зомби:настоящий:amod:adj',\n",
       " 'зомби:новый:amod:adj',\n",
       " 'зомби:первый:amod:adj',\n",
       " 'зомби:послушный:amod:adj',\n",
       " 'зомби:приближаться:amod:verb',\n",
       " 'зомби:рыжий:amod:adj']"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import dawg\n",
    "from morph import morph\n",
    "from typing import List, Set, Tuple\n",
    "\n",
    "\n",
    "def inflect(word: str, grs_variants: List[Set[str]]) -> Tuple[str, str]:\n",
    "    parsed = morph.parse(word)\n",
    "\n",
    "    for grs in grs_variants:\n",
    "        for p in parsed:\n",
    "            inflected = p.inflect(grs)\n",
    "\n",
    "            if inflected:\n",
    "                return inflected.word\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_dep(completion_dawg, root, seed=None):\n",
    "    keys = completion_dawg.keys(f\"{root}:\")\n",
    "    \n",
    "    if not keys:\n",
    "        return None\n",
    "    \n",
    "    if seed:\n",
    "        random.seed(seed)\n",
    "    \n",
    "    [root, dep, rel, pos] = random.choice(keys).split(\":\")\n",
    "\n",
    "    \n",
    "    if rel == 'amod' and pos == 'adj':\n",
    "        root_parsed = morph.parse(root)[0]\n",
    "        inflected_adj = inflect(dep, [{ \"ADJF\", root_parsed.tag.gender, root_parsed.tag.case }])\n",
    "        \n",
    "        if inflected_adj is None:\n",
    "            return None\n",
    "        \n",
    "        return f\"{inflected_adj} {root_parsed.word}\"\n",
    "    \n",
    "    if rel == 'amod' and pos == 'verb':\n",
    "        root_parsed = morph.parse(root)[0]\n",
    "        base_verb_grs = { 'PRTF', 'sing', root_parsed.tag.gender, root_parsed.tag.case }\n",
    "        \n",
    "        inflected_adj = inflect(dep, [{'actv', 'pres'} | base_verb_grs ,{'pssv', 'past'} | base_verb_grs, {'actv', 'past'} | base_verb_grs])\n",
    "        \n",
    "        if inflected_adj is None:\n",
    "            return None\n",
    "        return f\"{inflected_adj} {root_parsed.word}\"\n",
    "    \n",
    "    return None\n",
    "    \n",
    "# for i in range(0, 300):\n",
    "#     print(get_dep(completion_dawg, 'внучка', i))\n",
    "\n",
    "completion_dawg.keys(':')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b40be0d",
   "metadata": {},
   "source": [
    "# Распознание сущностей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beca5446",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
