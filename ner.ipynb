{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d437fae",
   "metadata": {},
   "source": [
    "# Сбор связей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a748fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymorphy2-dicts-ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fbbbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2811c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from nlp import nlp\n",
    "from dawg import CompletionDAWG\n",
    "import torch\n",
    "import stanza\n",
    "import glob\n",
    "\n",
    "\n",
    "def filter_deps(word):\n",
    "    return word.deprel in [\"amod\"] # , \"appos\"\n",
    "\n",
    "\n",
    "def find_deps(words):\n",
    "    deps = []\n",
    "    \n",
    "    for w in words:\n",
    "        if w.upos == \"NOUN\" and \"Animacy=Anim\" in w.feats:\n",
    "            deps.append((w, [rw for rw in words if rw.head == w.id and filter_deps(rw)]))\n",
    "    \n",
    "    return deps\n",
    "\n",
    "\n",
    "def format_dep(word):\n",
    "    return f\"{word.lemma.replace('-', '')}:{word.deprel}:{word.upos}\"\n",
    "\n",
    "\n",
    "def collect_deps(all_deps, deps):\n",
    "    for (root, dep_words) in deps:\n",
    "        if dep_words:\n",
    "            all_deps[root.lemma.replace('-', '')] += Counter({ format_dep(dw) for dw in dep_words })\n",
    "\n",
    "    \n",
    "def get_stanza_filenames():\n",
    "    return glob.iglob('/mnt/data/proza_ru/**/*.txt.stanza', recursive=True)\n",
    "\n",
    "\n",
    "def main():\n",
    "    deps_counter = defaultdict(Counter)\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for filename in get_stanza_filenames():\n",
    "            if i % 1000 == 0: print(i)\n",
    "            i += 1\n",
    "            \n",
    "            with open(filename, 'rb') as file:\n",
    "                try:\n",
    "                    doc = stanza.Document.from_serialized(file.read())\n",
    "\n",
    "                    for sent in doc.sentences:\n",
    "                        collect_deps(deps_counter, find_deps(sent.words))\n",
    "                except Exception as e:\n",
    "                    print(f\"rm {filename} &&\")\n",
    "\n",
    "    return deps_counter\n",
    "\n",
    "\n",
    "# deps = defaultdict(Counter)\n",
    "\n",
    "# for sent in doc.sentences:\n",
    "#     add_deps(deps, find_deps(sent.words))\n",
    "    \n",
    "\n",
    "\n",
    "# completion_dawg = CompletionDAWG([f\"{root}:{dep}\" for root in deps for dep in deps[root]])\n",
    "rels = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "fb7141c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from morph import morph\n",
    "from rapidfuzz.distance import JaroWinkler\n",
    "from itertools import takewhile\n",
    "import dawg\n",
    "\n",
    "\n",
    "def is_known(word):\n",
    "    for parse in morph.parse(word):\n",
    "        if parse.normal_form.replace('ё', 'е') == word.replace('ё', 'е') and 'nomn' in parse.tag and parse.is_known and ('anim' in parse.tag or 'ADJF' in parse.tag or \"PRTF\" in parse.tag):\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def normalize(word):\n",
    "    for parse in morph.parse(word):\n",
    "        if parse.is_known and ('anim' in parse.tag or 'ADJF' in parse.tag or \"PRTF\" in parse.tag):\n",
    "            return parse.normal_form.replace('ё', 'е')\n",
    "\n",
    "    return word.replace('ё', 'е')\n",
    "\n",
    "\n",
    "# Удалить руты с очен маленьким количеством связей\n",
    "# Удалить редкие депсы\n",
    "def merge_unknown_words_to_known(rels):\n",
    "    dict_roots = [root for root in rels if is_known(root)]\n",
    "    non_dict_roots = [root for root in rels if root not in dict_roots]\n",
    "\n",
    "    print(len(dict_roots), len(non_dict_roots))\n",
    "    \n",
    "    # Удаляем редкие неизвестные слова\n",
    "    # и совсем короткие\n",
    "    # и нормализуемые\n",
    "    for ndr in non_dict_roots:\n",
    "        vals = rels[ndr].values()\n",
    "        \n",
    "        if max(vals) < 3:\n",
    "            print('Удаляем по весу ', ndr, vals)\n",
    "            rels.pop(ndr)\n",
    "            non_dict_roots.remove(ndr)\n",
    "            \n",
    "        if len(ndr) < 3:\n",
    "            print('Удаляем по длине ', ndr, vals)\n",
    "            rels.pop(ndr)\n",
    "            non_dict_roots.remove(ndr)\n",
    "        \n",
    "        normalized_ndr = normalize(ndr)\n",
    "        if normalized_ndr != ndr and normalized_ndr in dict_roots:\n",
    "            print('Сливаем normalized', ndr, normalized_ndr)\n",
    "            rels[normalized_ndr] |= rels[ndr]\n",
    "            rels.pop(ndr) \n",
    "            non_dict_roots.remove(ndr)\n",
    "            \n",
    "        ndr_a = f'{ndr}а';\n",
    "        \n",
    "        if is_known(ndr_a):\n",
    "            print('Сливаем +a', ndr, ndr_a)\n",
    "            rels[ndr_a] = rels.get(ndr_a, Counter())\n",
    "            rels[ndr_a] |= rels[ndr]\n",
    "            rels.pop(ndr)\n",
    "            non_dict_roots.remove(ndr)\n",
    "        \n",
    "            \n",
    "    # Сливаем похожие слова\n",
    "    for ndr in non_dict_roots:\n",
    "        nearest_dr = None\n",
    "        nearest_dr_sim = 0\n",
    "        \n",
    "        for dr in dict_roots:\n",
    "            sim = JaroWinkler.similarity(ndr, dr)\n",
    "            \n",
    "            if sim < 0.96:\n",
    "                continue\n",
    "                \n",
    "            if sim > nearest_dr_sim:\n",
    "                nearest_dr_sim = sim\n",
    "                nearest_dr = dr\n",
    "        \n",
    "        if nearest_dr:\n",
    "            print('Сливаем близкие ', ndr, nearest_dr)\n",
    "            rels[nearest_dr] |= rels[ndr]\n",
    "            rels.pop(ndr)\n",
    "            \n",
    "    print(len(dict_roots), len(non_dict_roots))\n",
    "        \n",
    "\n",
    "def drop_rare_roots(rels):\n",
    "    rare = [root for root in rels if max(rels[root].values()) < 3]\n",
    "    \n",
    "    for r in rare:\n",
    "        print('drop_rare_roots ', r, max(rels[r].values()))\n",
    "        rels.pop(r)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "# merge_unknown_words_to_known(rels)\n",
    "\n",
    "\n",
    "def rels_to_seq(rels):\n",
    "    for root in rels:\n",
    "        if not is_known(root):\n",
    "            continue\n",
    "            \n",
    "        deps = rels[root]\n",
    "        bound = 0\n",
    "        \n",
    "        if len(deps) > 2:\n",
    "            [common1, common2] = deps.most_common(2)\n",
    "            bound = common2[1] / 10\n",
    "    \n",
    "        for dep in deps:\n",
    "#            местоимение\n",
    "            [dep_word, dep_rel, dep_pos] = dep.split(\":\")\n",
    "            if 'Apro' in morph.parse(dep_word)[0].tag:\n",
    "                continue\n",
    "                \n",
    "            if 'блуд' in dep_word or 'мертв' in dep_word or 'твой' in dep_word or 'сбит' in dep_word or 'убит' in dep_word: \n",
    "                continue\n",
    "            \n",
    "            if deps[dep] >= bound:\n",
    "                yield f\"{root.lower().replace('ё', 'е')}:{dep.lower().replace('ё', 'е')}\"\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "completion_dawg = dawg.CompletionDAWG(rels_to_seq(rels))\n",
    "\n",
    "completion_dawg.save('/mnt/data/adj.dawg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "402e7360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='спятившая', tag=OpencorporaTag('PRTF,perf,intr,past,actv femn,sing,nomn'), normal_form='спятить', score=1.0, methods_stack=((DictionaryAnalyzer(), 'спятившая', 2345, 22),))]"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph.parse('спятившая')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "8a9f5ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['зомби:мерзкий:amod:adj',\n",
       " 'зомби:настоящий:amod:adj',\n",
       " 'зомби:новый:amod:adj',\n",
       " 'зомби:первый:amod:adj',\n",
       " 'зомби:послушный:amod:adj',\n",
       " 'зомби:приближаться:amod:verb',\n",
       " 'зомби:рыжий:amod:adj']"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import dawg\n",
    "from morph import morph\n",
    "from typing import List, Set, Tuple\n",
    "\n",
    "\n",
    "def inflect(word: str, grs_variants: List[Set[str]]) -> Tuple[str, str]:\n",
    "    parsed = morph.parse(word)\n",
    "\n",
    "    for grs in grs_variants:\n",
    "        for p in parsed:\n",
    "            inflected = p.inflect(grs)\n",
    "\n",
    "            if inflected:\n",
    "                return inflected.word\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_dep(completion_dawg, root, seed=None):\n",
    "    keys = completion_dawg.keys(f\"{root}:\")\n",
    "    \n",
    "    if not keys:\n",
    "        return None\n",
    "    \n",
    "    if seed:\n",
    "        random.seed(seed)\n",
    "    \n",
    "    [root, dep, rel, pos] = random.choice(keys).split(\":\")\n",
    "\n",
    "    \n",
    "    if rel == 'amod' and pos == 'adj':\n",
    "        root_parsed = morph.parse(root)[0]\n",
    "        inflected_adj = inflect(dep, [{ \"ADJF\", root_parsed.tag.gender, root_parsed.tag.case }])\n",
    "        \n",
    "        if inflected_adj is None:\n",
    "            return None\n",
    "        \n",
    "        return f\"{inflected_adj} {root_parsed.word}\"\n",
    "    \n",
    "    if rel == 'amod' and pos == 'verb':\n",
    "        root_parsed = morph.parse(root)[0]\n",
    "        base_verb_grs = { 'PRTF', 'sing', root_parsed.tag.gender, root_parsed.tag.case }\n",
    "        \n",
    "        inflected_adj = inflect(dep, [{'actv', 'pres'} | base_verb_grs ,{'pssv', 'past'} | base_verb_grs, {'actv', 'past'} | base_verb_grs])\n",
    "        \n",
    "        if inflected_adj is None:\n",
    "            return None\n",
    "        return f\"{inflected_adj} {root_parsed.word}\"\n",
    "    \n",
    "    return None\n",
    "    \n",
    "# for i in range(0, 300):\n",
    "#     print(get_dep(completion_dawg, 'внучка', i))\n",
    "\n",
    "completion_dawg.keys(':')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b40be0d",
   "metadata": {},
   "source": [
    "# Распознание сущностей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc81bb53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
