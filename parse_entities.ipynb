{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b0bcb0f",
   "metadata": {},
   "source": [
    "https://universaldependencies.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0910c644",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install DAWG stanza multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7483d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a58fba6",
   "metadata": {},
   "source": [
    "# Разбор синтаксиса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4dbcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 starts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3b5fd46bf65425aa9128aa18bcfe2a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-03 07:55:46 INFO: Downloading these customized packages for language: ru (Russian)...\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | syntagrus |\n",
      "| pos       | syntagrus |\n",
      "| lemma     | syntagrus |\n",
      "| depparse  | syntagrus |\n",
      "| pretrain  | syntagrus |\n",
      "=========================\n",
      "\n",
      "2022-03-03 07:55:46 INFO: File exists: /home/kukuruku/stanza_resources/ru/tokenize/syntagrus.pt.\n",
      "2022-03-03 07:55:46 INFO: File exists: /home/kukuruku/stanza_resources/ru/pos/syntagrus.pt.\n",
      "2022-03-03 07:55:46 INFO: File exists: /home/kukuruku/stanza_resources/ru/lemma/syntagrus.pt.\n",
      "2022-03-03 07:55:48 INFO: File exists: /home/kukuruku/stanza_resources/ru/depparse/syntagrus.pt.\n",
      "2022-03-03 07:55:49 INFO: File exists: /home/kukuruku/stanza_resources/ru/pretrain/syntagrus.pt.\n",
      "2022-03-03 07:55:49 INFO: Finished downloading models and saved to /home/kukuruku/stanza_resources.\n",
      "2022-03-03 07:55:49 INFO: Loading these models for language: ru (Russian):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | syntagrus |\n",
      "| pos       | syntagrus |\n",
      "| lemma     | syntagrus |\n",
      "| depparse  | syntagrus |\n",
      "=========================\n",
      "\n",
      "2022-03-03 07:55:50 INFO: Use device: gpu\n",
      "2022-03-03 07:55:50 INFO: Loading: tokenize\n",
      "2022-03-03 07:56:19 INFO: Loading: pos\n",
      "2022-03-03 07:56:20 INFO: Loading: lemma\n",
      "2022-03-03 07:56:20 INFO: Loading: depparse\n",
      "2022-03-03 07:56:20 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 work\n",
      "0 81300\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import stanza\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display, clear_output\n",
    "import multiprocessing\n",
    "import time\n",
    "import torch\n",
    "\n",
    "def get_filenames():\n",
    "    return glob.iglob('/mnt/notebooks-a/home/kukuruku/data/proza_ru/**/*.txt', recursive=True)\n",
    "\n",
    "\n",
    "FILES_COUNT = 10000000\n",
    "\n",
    "\n",
    "def process_text(p):\n",
    "#     progress = IntProgress(min=0, max=FILES_COUNT) # instantiate the bar\n",
    "#     display(progress)\n",
    "    \n",
    "    skip_num = p * 30000\n",
    "    retries = 0\n",
    "    \n",
    "    time.sleep(p*30)\n",
    "    print(f\"{p} starts\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            stanza.download('ru', processors=\"tokenize,pos,lemma,depparse\")\n",
    "            nlp = stanza.Pipeline('ru', processors=\"tokenize,pos,lemma,depparse\")\n",
    "                \n",
    "            break\n",
    "        except Exception:\n",
    "            retries += 1\n",
    "            time.sleep(p*2 + 10)\n",
    "            \n",
    "            if retries > 10:\n",
    "                print(f\"{p} cannot load nlp\")\n",
    "                return\n",
    "            \n",
    "            print('Retry load nlp')\n",
    "\n",
    "\n",
    "    i = 0\n",
    "    print(f\"{p} work\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for file_name in get_filenames():\n",
    "            i += 1\n",
    "            if i < skip_num: continue\n",
    "\n",
    "            stanza_file_name = f\"{file_name}.stanza\"\n",
    "\n",
    "            if os.path.isfile(stanza_file_name):\n",
    "                continue\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(p, i)\n",
    "\n",
    "    #         progress.value = i\n",
    "    #         progress.description = f\"p {p}: {i} of {FILES_COUNT}\"\n",
    "            try:\n",
    "               with open(file_name, 'r') as file:\n",
    "                    with open(stanza_file_name, 'wb') as stanza_file:\n",
    "                        text = file.read()\n",
    "                        doc = nlp(text)\n",
    "                        stanza_file.write(doc.to_serialized())\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                torch.cuda.empty_cache()\n",
    "                continue\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     print('Main tread: start workers')\n",
    "#     jobs = []\n",
    "#     for i in range(4):\n",
    "#         p = multiprocessing.Process(target=process_text, args=(i,))\n",
    "#         jobs.append(p)\n",
    "#         p.start()\n",
    "\n",
    "process_text(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abc7dba",
   "metadata": {},
   "source": [
    "# Чтерие готовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51b043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import stanza\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "if not 'nlp' in locals():\n",
    "    stanza.download('ru')\n",
    "    nlp = stanza.Pipeline('ru')\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    \n",
    "file_names = glob.iglob('/home/kukuruku/data/proza_ru/**/*.txt', recursive=True)\n",
    "    \n",
    "    \n",
    "for file_name in file_names:\n",
    "    stanza_file_name = f\"{file_name}.stanza\"\n",
    "    \n",
    "    try:   \n",
    "        with open(stanza_file_name, 'rb') as stanza_file:\n",
    "            doc = stanza.Document.from_serialized(stanza_file.read())\n",
    "            \n",
    "            print(doc)\n",
    "    except Exception:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42055a58",
   "metadata": {},
   "source": [
    "# Распознание сущностей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4801fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "if not 'nlp' in locals():\n",
    "    stanza.download('ru')\n",
    "    nlp = stanza.Pipeline('ru')\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    \n",
    "\n",
    "def find_root_entity(words):\n",
    "    root = None\n",
    "    amods = []\n",
    "    \n",
    "    for w in words:\n",
    "        if w.upos == \"NOUN\" and w.deprel == \"root\":\n",
    "            root = w\n",
    "            break\n",
    "            \n",
    "    if not root:\n",
    "        return None\n",
    "    \n",
    "    for w in words:\n",
    "        if w.deprel == \"amod\" and w.head == root.id:\n",
    "            amods.append(w)\n",
    "\n",
    "    return ' '.join([*[a.text for a in amods], root.text])\n",
    "\n",
    "\n",
    "assert find_root_entity(nlp('белая курица').sentences[0].words) == 'белая курица'\n",
    "assert find_root_entity(nlp('рябина красная').sentences[0].words) == 'красная рябина'\n",
    "\n",
    "nlp('застежка липучка')\n",
    "# assert find_root_entity(nlp('застежка липучка').sentences[0].words) == 'застежка липучка'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31452e91",
   "metadata": {},
   "source": [
    "import dawg\n",
    "\n",
    "words = ['hello', 'world', 'hello world', 'hello dawg']\n",
    "\n",
    "base_dawg = dawg.DAWG(words)\n",
    "completion_dawg = dawg.CompletionDAWG(words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
